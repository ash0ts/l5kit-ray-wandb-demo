{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2aXGGrVOJCbd"
   },
   "source": [
    "## Prepare Data path and load cfg\n",
    "\n",
    "By setting the `L5KIT_DATA_FOLDER` variable, we can point the script to the folder where the data lies.\n",
    "\n",
    "Then, we load our config file with relative paths and other configurations (rasteriser, training params...)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: DONT USE RELATIVE PATHS FOR THE MODELS PROVIDED BY L5\n",
    "experiments_directory = Path(Path(os.path.abspath('')).parent.parent, \"Experiments\")\n",
    "experiments_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "data_directory = Path(experiments_directory, \"data\")\n",
    "data_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "prediction_directory = Path(experiments_directory, \"prediction\")\n",
    "prediction_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "prediction_training_directory = Path(prediction_directory, \"training\")\n",
    "prediction_training_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "save_directory = Path(prediction_training_directory, \"saved_outputs\")\n",
    "save_directory.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(prediction_training_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "l5kit\n",
    "pyyaml\n",
    "ray==2.0.0rc1\n",
    "ray[air]\n",
    "wandb\n",
    "optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OKx_V_wnJCbb"
   },
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.models.resnet import resnet50\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset, EgoDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.evaluation import write_pred_csv, compute_metrics_csv, read_gt_csv, create_chopped_dataset\n",
    "from l5kit.evaluation.chop_dataset import MIN_FUTURE_STEPS\n",
    "from l5kit.evaluation.metrics import neg_multi_log_likelihood, time_displace, rmse, prob_true_mode, average_displacement_error_oracle, average_displacement_error_mean, final_displacement_error_oracle, final_displacement_error_mean, detect_collision, distance_to_reference_trajectory\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import PREDICTED_POINTS_COLOR, TARGET_POINTS_COLOR, draw_trajectory\n",
    "from prettytable import PrettyTable\n",
    "from pathlib import Path\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Data from Wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "executionInfo": {
     "elapsed": 5572,
     "status": "ok",
     "timestamp": 1660707075761,
     "user": {
      "displayName": "Anish Shah",
      "userId": "05913492621931233323"
     },
     "user_tz": 240
    },
    "id": "Hbg_JE1s0q6u",
    "outputId": "48fec4f3-85e5-413a-911c-e4db3ad1aaa4"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run information\n",
    "# wandb_entity = \"l5-demo\"\n",
    "project_name = \"l5-prediction\"\n",
    "run_name = \"download-l5-data\"\n",
    "run_type = \"download\"\n",
    "run_description = \"\"\"\n",
    "Download data for the task of training a prediction model\n",
    "\"\"\"\n",
    "tags = [\"download\", \"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#🪄🐝\n",
    "run = wandb.init(\n",
    "    # entity=wandb_entity,\n",
    "    project=project_name,\n",
    "    job_type=run_type,\n",
    "    name=run_name,\n",
    "    notes=run_description,\n",
    "    tags=tags\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artifact_entity = \"l5-demo\"\n",
    "artifact_project = \"l5-common\"\n",
    "artifact_name = \"l5-data\"\n",
    "artifact_alias = \"latest\"\n",
    "artifact_type = \"dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#🪄🐝\n",
    "# artifact = run.use_artifact(f\"{artifact_entity}/{artifact_project}/{artifact_name}:{artifact_alias}\", type=artifact_type)\n",
    "artifact = run.use_artifact(f\"{artifact_project}/{artifact_name}:{artifact_alias}\", type=artifact_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact l5-data:latest, 2386.92MB. 517 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   517 of 517 files downloaded.  \n",
      "Done. 0:0:0.2\n"
     ]
    }
   ],
   "source": [
    "_ = artifact.download(data_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUG: need to seperate runs into download and training due to issues with routing runs after ray.tune\n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset is assumed to be on the folder specified\n",
    "# in the L5KIT_DATA_FOLDER environment variable\n",
    "\n",
    "# get config\n",
    "cfg = load_config_data(Path(data_directory, \"configurations\", \"agent_motion_config.yaml\"))\n",
    "l5_data_location = Path(data_directory, \"dataset\")\n",
    "# run.config.update(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfg[\"zarr_dataset_location\"] = l5_data_location\n",
    "os.environ[\"L5KIT_DATA_FOLDER\"] = str(l5_data_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ZX_d_ObJCbg"
   },
   "source": [
    "## Model\n",
    "\n",
    "Our baseline is a simple `resnet50` pretrained on `imagenet`. We must replace the input and the final layer to address our requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2i3FeP99JCbg"
   },
   "outputs": [],
   "source": [
    "def build_model(cfg: Dict) -> torch.nn.Module:\n",
    "    # load pre-trained Conv2D model\n",
    "    model = resnet50(pretrained=True)\n",
    "\n",
    "    # change input channels number to match the rasterizer's output\n",
    "    num_history_channels = (cfg[\"model_params\"][\"history_num_frames\"] + 1) * 2\n",
    "    num_in_channels = 3 + num_history_channels\n",
    "    model.conv1 = nn.Conv2d(\n",
    "        num_in_channels,\n",
    "        model.conv1.out_channels,\n",
    "        kernel_size=model.conv1.kernel_size,\n",
    "        stride=model.conv1.stride,\n",
    "        padding=model.conv1.padding,\n",
    "        bias=False,\n",
    "    )\n",
    "    # change output size to (X, Y) * number of future states\n",
    "    num_targets = 2 * cfg[\"model_params\"][\"future_num_frames\"]\n",
    "    model.fc = nn.Linear(in_features=2048, out_features=num_targets)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-MMuG4LJCbh"
   },
   "outputs": [],
   "source": [
    "def forward(data, model, criterion):\n",
    "    inputs = data[\"image\"]\n",
    "    target_availabilities = data[\"target_availabilities\"].unsqueeze(-1)\n",
    "    targets = data[\"target_positions\"]\n",
    "    # Forward pass\n",
    "    outputs = model(inputs).reshape(targets.shape)\n",
    "    loss = criterion(outputs, targets)\n",
    "    # not all the output steps are valid, but we can filter them out from the loss using availabilities\n",
    "    loss = loss * target_availabilities\n",
    "    loss = loss.mean()\n",
    "    return loss, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I8zTOjoAz3mW"
   },
   "outputs": [],
   "source": [
    "def train_prediction_model_epoch(data, model, criterion, optimizer):\n",
    "    loss, outputs = forward(data, model, criterion)\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss, outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oz8fUbbTJCbh"
   },
   "source": [
    "Our data pipeline map a raw `.zarr` folder into a multi-processing instance ready for training by:\n",
    "- loading the `zarr` into a `ChunkedDataset` object. This object has a reference to the different arrays into the zarr (e.g. agents and traffic lights);\n",
    "- wrapping the `ChunkedDataset` into an `AgentDataset`, which inherits from torch `Dataset` class;\n",
    "- passing the `AgentDataset` into a torch `DataLoader`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EDp7WnqJCbh"
   },
   "source": [
    "# Training\n",
    "\n",
    "note: if you're on MacOS and using `py_satellite` rasterizer, you may need to disable opencv multiprocessing by adding:\n",
    "`cv2.setNumThreads(0)` before the following cell. This seems to only affect running in python notebook and it's caused by the `cv2.warpaffine` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQGV6T8F1q8h"
   },
   "outputs": [],
   "source": [
    "import ray.train as train\n",
    "from ray.air import session, Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune.tuner import Tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8gRu7-HmxCrE"
   },
   "outputs": [],
   "source": [
    "def train_prediction_model(tuner_cfg : Dict):\n",
    "    trial_name = session.get_trial_name()\n",
    "    trial_id = session.get_trial_id()\n",
    "    trial_readable_name = f\"{trial_name}_{trial_id}\"\n",
    "    \n",
    "    dm = LocalDataManager()\n",
    "    \n",
    "    # ==== Configurations\n",
    "    shuffle = tuner_cfg[\"shuffle\"]\n",
    "    batch_size = int(tuner_cfg[\"batch_size\"])\n",
    "    num_workers = tuner_cfg[\"num_workers\"]\n",
    "    lr = tuner_cfg[\"lr\"]\n",
    "    max_num_steps = int(tuner_cfg[\"max_num_steps\"])\n",
    "    dataset_key = tuner_cfg[\"dataset_key\"]\n",
    "    cfg = tuner_cfg[\"cfg\"]\n",
    "    \n",
    "    # ==== Loading Dataset\n",
    "    rasterizer = build_rasterizer(cfg, dm)\n",
    "\n",
    "    train_zarr = ChunkedDataset(dm.require(dataset_key)).open()\n",
    "    train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "\n",
    "    batch_size_per_worker = batch_size // session.get_world_size()\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=shuffle, batch_size=batch_size_per_worker, num_workers=num_workers)\n",
    "    train_dataloader = train.torch.prepare_data_loader(train_dataloader)\n",
    "    \n",
    "    # ==== Init model\n",
    "    model = build_model(cfg)\n",
    "    model = train.torch.prepare_model(model)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss(reduction=\"none\")\n",
    "\n",
    "    # ==== TRAIN LOOP\n",
    "    tr_it = iter(train_dataloader)\n",
    "    progress_bar = range(max_num_steps)\n",
    "    num_checkpoints = 5\n",
    "    steps_before_checkpointing = max_num_steps // num_checkpoints\n",
    "    losses_train = []\n",
    "    checkpoint_counter = 0\n",
    "    \n",
    "    for step in progress_bar:\n",
    "        try:\n",
    "            data = next(tr_it)\n",
    "        except StopIteration:\n",
    "            tr_it = iter(train_dataloader)\n",
    "            data = next(tr_it)\n",
    "            \n",
    "        model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "        loss, _ = train_prediction_model_epoch(data, model, criterion, optimizer)\n",
    "        losses_train.append(loss.item())\n",
    "        avg_loss = np.mean(losses_train)\n",
    "        metrics = {\n",
    "            \"loss\": loss.item(),\n",
    "            \"avg_loss\": avg_loss\n",
    "        }\n",
    "        \n",
    "        if train.world_rank() == 0:\n",
    "            print(metrics)\n",
    "        \n",
    "        if (step%steps_before_checkpointing==0) or (step==max_num_steps-1):\n",
    "            session.report(\n",
    "                metrics=metrics,\n",
    "                checkpoint=Checkpoint.from_dict(dict(step=step, model=model)))\n",
    "            checkpoint_counter += 1\n",
    "        else:\n",
    "            session.report(\n",
    "                metrics=metrics\n",
    "            )\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Training using Ray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We calculate the available hardware for our current training sessions and efficiently split CPUs based on GPUs or split CPUs evenly if possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2ic0NRa0X2q"
   },
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchTrainer\n",
    "from ray.air.config import RunConfig, ScalingConfig\n",
    "from ray.air.callbacks.wandb import WandbLoggerCallback #🪄🐝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU = torch.cuda.is_available()\n",
    "NUM_GPUS = torch.cuda.device_count()\n",
    "NUM_CPUS = multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_GPU:\n",
    "    num_actors = NUM_GPUS\n",
    "    num_data_workers = NUM_CPUS // num_actors\n",
    "else:\n",
    "    num_data_workers = 4 if NUM_CPUS>=4 else NUM_CPUS\n",
    "    ideal_num_actors = NUM_CPUS // num_data_workers\n",
    "    num_actors = ideal_num_actors if ideal_num_actors else 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use Ray all we need to simply do is wrap the training function above. The only addition needed above was calls to `report.session` to log metrics during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6465,
     "status": "ok",
     "timestamp": 1660707090583,
     "user": {
      "displayName": "Anish Shah",
      "userId": "05913492621931233323"
     },
     "user_tz": 240
    },
    "id": "MqgEP_OE0cIa",
    "outputId": "941a3d0a-e9a6-4f8a-dd89-5c5414e63555"
   },
   "outputs": [],
   "source": [
    "#NOTE: To figure out if scaling config intuiutin is correct: num_actors divide resources between each actor and within the train func each actor can the utilize the shared resources\n",
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_prediction_model,\n",
    "    scaling_config=ScalingConfig(num_workers=num_actors, use_gpu=USE_GPU),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributed Hyperparemeter Tuning using Ray\n",
    "\n",
    "Due to Ray's easy interface we can simply extend our normal trainer to Ray's tuner which will allow us to do efficient hyperparameter optimization. In our case we use `optuna`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_train_config = {}\n",
    "##static\n",
    "tuner_train_config[\"shuffle\"] = cfg[\"train_data_loader\"][\"shuffle\"]\n",
    "tuner_train_config[\"num_workers\"] = num_data_workers\n",
    "tuner_train_config[\"dataset_key\"] = cfg[\"train_data_loader\"][\"key\"]\n",
    "\n",
    "##tunable\n",
    "tuner_train_config[\"max_num_steps\"] = tune.quniform(1000, 5000, 250)\n",
    "tuner_train_config[\"lr\"] = tune.loguniform(1e-4, 1e-2)\n",
    "tuner_train_config[\"batch_size\"] = tune.quniform(6, 24, 6)\n",
    "cfg[\"raster_params\"][\"map_type\"] = tune.choice([\"py_semantic\", \"py_satellite\"])\n",
    "\n",
    "tuner_train_config[\"cfg\"] = cfg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.logger import LoggerCallback\n",
    "from typing import Dict, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.tune.stopper import ExperimentPlateauStopper\n",
    "from ray.tune.search.optuna import OptunaSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_search_attempts = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optuna_search = OptunaSearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "        trainer,\n",
    "        tune_config=tune.TuneConfig(\n",
    "            metric=\"avg_loss\", #loss or avg_loss here?\n",
    "            mode=\"min\",\n",
    "            search_alg=optuna_search,\n",
    "            num_samples=n_search_attempts,\n",
    "        ),\n",
    "        param_space={\n",
    "            \"train_loop_config\": tuner_train_config\n",
    "        },\n",
    "        run_config=RunConfig(\n",
    "            stop=ExperimentPlateauStopper(\"avg_loss\"),\n",
    "            callbacks=[WandbLoggerCallback(project=f\"{project_name}-trials\", save_checkpoints=True),]))  #🪄🐝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregrate and Report Metrics from All Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 158825,
     "status": "ok",
     "timestamp": 1660707249406,
     "user": {
      "displayName": "Anish Shah",
      "userId": "05913492621931233323"
     },
     "user_tz": 240
    },
    "id": "kOE9cNax197M",
    "outputId": "ff23d83a-4e9f-4876-f1cd-54f752b4563d"
   },
   "outputs": [],
   "source": [
    "analysis = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df = analysis.get_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run information\n",
    "# wandb_entity = \"l5-demo\"\n",
    "project_name = \"l5-prediction\"\n",
    "run_name = \"train-prediction-model\"\n",
    "run_type = \"train\"\n",
    "run_description = \"\"\"\n",
    "Train prediction model\n",
    "\"\"\"\n",
    "tags = [\"train\", \"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#🪄🐝\n",
    "run = wandb.init(\n",
    "    # entity=wandb_entity,\n",
    "    project=project_name,\n",
    "    job_type=run_type,\n",
    "    name=run_name,\n",
    "    notes=run_description,\n",
    "    tags=tags,\n",
    "    config=cfg\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUG: to force a connection on the lineage graph\n",
    "#🪄🐝\n",
    "# artifact = run.use_artifact(f\"{artifact_entity}/{artifact_project}/{artifact_name}:{artifact_alias}\", type=artifact_type)\n",
    "artifact = run.use_artifact(f\"{artifact_project}/{artifact_name}:{artifact_alias}\", type=artifact_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#🪄🐝\n",
    "analysis_table = wandb.Table(dataframe=analysis_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUG: run gets lost after tune job due to change in cwd. Forced to make 2 runs\n",
    "if len(analysis_table.data) == 0:\n",
    "    raise ValueError(\"bad table for some reason\")\n",
    "else:\n",
    "    run.log({\"analysis_table\": analysis_table})\n",
    "    run.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Level-5-agent_motion_prediction.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/lyft/l5kit/blob/master/examples/agent_motion_prediction/agent_motion_prediction.ipynb",
     "timestamp": 1660678336545
    }
   ]
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m103"
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
