{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I8qWX2klDmRx"
   },
   "source": [
    "# ML Simulation Training\n",
    "In this notebook you are going to train a ML policy.\n",
    "\n",
    "However, you won't use examples from the SDV as data, but other agents around it instead.\n",
    "\n",
    "This may sound like a small difference, but it has profound implications still:\n",
    "- by using data from multiple sources you're **including much more variability in your training data**;\n",
    "- two agents may have taken different choices at the same intersection, leading to **multi-modal data**;\n",
    "- the **quality of the annotated data is expected to be sensibility lower** compared to the SDV, as we're leveraging a perception system.\n",
    "\n",
    "Still, the final prize is even better than planning as this policy can potentially drive all the agents in the scene and it's not limited to the SDV only.\n",
    "\n",
    "![simulation-example](https://github.com/woven-planet/l5kit/blob/master/docs/images/simulation/simulation_example.svg?raw=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R5iNJ7flt5rO"
   },
   "outputs": [],
   "source": [
    "%%writefile setup_notebook_colab.sh\n",
    "#!/bin/bash\n",
    "\n",
    "# Make a temporary download folder\n",
    "TEMP_DOWNLOAD_DIR=$(mktemp -d)\n",
    "TEMP_DATASET_DIR=$(mktemp -d)\n",
    "\n",
    "# Download sample zarr\n",
    "echo \"Downloading sample zarr dataset...\"\n",
    "wget https://lyft-l5-datasets-public.s3-us-west-2.amazonaws.com/prediction/v1.1/sample.tar \\\n",
    "    -q --show-progress -P $TEMP_DOWNLOAD_DIR\n",
    "\n",
    "mkdir -p $TEMP_DATASET_DIR/scenes\n",
    "tar xf $TEMP_DOWNLOAD_DIR/sample.tar -C $TEMP_DATASET_DIR/scenes\n",
    "\n",
    "# Download semantic map\n",
    "echo \"Downloading semantic map...\"\n",
    "wget https://lyft-l5-datasets-public.s3-us-west-2.amazonaws.com/prediction/v1.1/semantic_map.tar \\\n",
    "    -q --show-progress -P $TEMP_DOWNLOAD_DIR\n",
    "mkdir -p $TEMP_DATASET_DIR/semantic_map\n",
    "tar xf $TEMP_DOWNLOAD_DIR/semantic_map.tar -C $TEMP_DATASET_DIR/semantic_map\n",
    "cp $TEMP_DATASET_DIR/semantic_map/meta.json $TEMP_DATASET_DIR/meta.json\n",
    "\n",
    "wget https://raw.githubusercontent.com/woven-planet/l5kit/master/examples/simulation/config.yaml -q\n",
    "\n",
    "# Install L5Kit\n",
    "echo \"Installing L5kit...\"\n",
    "pip install --progress-bar off --quiet -U l5kit pyyaml\n",
    "pip install ray==2.0.0rc1 --quiet\n",
    "pip install \"ray[air]\" --quiet\n",
    "pip install -U wandb --quiet\n",
    "\n",
    "echo \"Dataset and L5kit are ready !\"\n",
    "echo $TEMP_DATASET_DIR > \"dataset_dir.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 61890,
     "status": "ok",
     "timestamp": 1660683905851,
     "user": {
      "displayName": "Anish Shah",
      "userId": "05913492621931233323"
     },
     "user_tz": 240
    },
    "id": "ie7xWJxxDmR2",
    "outputId": "bcd3515c-ebef-436a-ebc8-1a002ce83442"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading sample zarr dataset...\n",
      "sample.tar          100%[===================>]  51.37M  24.6MB/s    in 2.1s    \n",
      "Downloading semantic map...\n",
      "semantic_map.tar    100%[===================>]   2.86M  4.52MB/s    in 0.6s    \n",
      "Installing L5kit...\n",
      "\u001b[?25l\n",
      "\u001b[?25h\u001b[?25l\n",
      "\u001b[?25h\u001b[?25l\n",
      "\u001b[?25h\u001b[?25l\n",
      "\u001b[?25h\u001b[?25l\n",
      "\u001b[?25h\u001b[?25l\n",
      "\u001b[?25h\u001b[?25l\n",
      "\u001b[?25h\u001b[?25l\n",
      "\u001b[?25h\u001b[?25l\n",
      "\u001b[?25h  Building wheel for ptable (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for transforms3d (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for asciitree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\n",
      "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\n",
      "cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.19.5 which is incompatible.\u001b[0m\n",
      "Dataset and L5kit are ready !\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting ray==2.0.0rc1\n",
      "  Downloading ray-2.0.0rc1-cp37-cp37m-manylinux2014_x86_64.whl (59.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 59.4 MB 1.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0rc1) (22.1.0)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0rc1) (3.17.3)\n",
      "Collecting grpcio<=1.43.0,>=1.28.1\n",
      "  Downloading grpcio-1.43.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 4.1 MB 49.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0rc1) (4.1.1)\n",
      "Requirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0rc1) (7.1.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0rc1) (6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0rc1) (3.8.0)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0rc1) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0rc1) (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0rc1) (2.23.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0rc1) (1.0.4)\n",
      "Collecting virtualenv\n",
      "  Downloading virtualenv-20.16.3-py2.py3-none-any.whl (8.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.8 MB 56.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0rc1) (4.3.3)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.7/dist-packages (from ray==2.0.0rc1) (1.3.1)\n",
      "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from grpcio<=1.43.0,>=1.28.1->ray==2.0.0rc1) (1.15.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray==2.0.0rc1) (5.9.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray==2.0.0rc1) (4.12.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray==2.0.0rc1) (0.18.1)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray==2.0.0rc1) (3.8.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray==2.0.0rc1) (1.24.3)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->ray==2.0.0rc1) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray==2.0.0rc1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray==2.0.0rc1) (2022.6.15)\n",
      "Collecting distlib<1,>=0.3.5\n",
      "  Downloading distlib-0.3.5-py2.py3-none-any.whl (466 kB)\n",
      "\u001b[K     |████████████████████████████████| 466 kB 64.7 MB/s \n",
      "\u001b[?25hCollecting platformdirs<3,>=2.4\n",
      "  Downloading platformdirs-2.5.2-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: platformdirs, distlib, virtualenv, grpcio, ray\n",
      "  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.47.0\n",
      "    Uninstalling grpcio-1.47.0:\n",
      "      Successfully uninstalled grpcio-1.47.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.8.2+zzzcolab20220719082949 requires numpy>=1.20, but you have numpy 1.19.5 which is incompatible.\u001b[0m\n",
      "Successfully installed distlib-0.3.5 grpcio-1.43.0 platformdirs-2.5.2 ray-2.0.0rc1 virtualenv-20.16.3\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: ray[air] in /usr/local/lib/python3.7/dist-packages (2.0.0rc1)\n",
      "Requirement already satisfied: aiosignal in /usr/local/lib/python3.7/dist-packages (from ray[air]) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.7/dist-packages (from ray[air]) (1.19.5)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from ray[air]) (3.8.0)\n",
      "Requirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from ray[air]) (22.1.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from ray[air]) (6.0)\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ray[air]) (1.0.4)\n",
      "Requirement already satisfied: frozenlist in /usr/local/lib/python3.7/dist-packages (from ray[air]) (1.3.1)\n",
      "Requirement already satisfied: virtualenv in /usr/local/lib/python3.7/dist-packages (from ray[air]) (20.16.3)\n",
      "Requirement already satisfied: grpcio<=1.43.0,>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from ray[air]) (1.43.0)\n",
      "Requirement already satisfied: jsonschema in /usr/local/lib/python3.7/dist-packages (from ray[air]) (4.3.3)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from ray[air]) (4.1.1)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from ray[air]) (2.23.0)\n",
      "Requirement already satisfied: click<=8.0.4,>=7.0 in /usr/local/lib/python3.7/dist-packages (from ray[air]) (7.1.2)\n",
      "Requirement already satisfied: protobuf<4.0.0,>=3.15.3 in /usr/local/lib/python3.7/dist-packages (from ray[air]) (3.17.3)\n",
      "Requirement already satisfied: pyarrow<7.0.0,>=6.0.1 in /usr/local/lib/python3.7/dist-packages (from ray[air]) (6.0.1)\n",
      "Collecting colorful\n",
      "  Downloading colorful-0.5.4-py2.py3-none-any.whl (201 kB)\n",
      "\u001b[K     |████████████████████████████████| 201 kB 5.1 MB/s \n",
      "\u001b[?25hCollecting starlette\n",
      "  Downloading starlette-0.20.4-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 2.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.7/dist-packages (from ray[air]) (3.8.1)\n",
      "Collecting gpustat>=1.0.0b1\n",
      "  Downloading gpustat-1.0.0rc1.tar.gz (89 kB)\n",
      "\u001b[K     |████████████████████████████████| 89 kB 9.0 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from ray[air]) (1.3.5)\n",
      "Collecting py-spy>=0.2.0\n",
      "  Downloading py_spy-0.3.12-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.1 MB 61.8 MB/s \n",
      "\u001b[?25hRequirement already satisfied: pydantic in /usr/local/lib/python3.7/dist-packages (from ray[air]) (1.9.1)\n",
      "Collecting prometheus-client<0.14.0,>=0.7.1\n",
      "  Downloading prometheus_client-0.13.1-py3-none-any.whl (57 kB)\n",
      "\u001b[K     |████████████████████████████████| 57 kB 5.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (from ray[air]) (0.8.10)\n",
      "Requirement already satisfied: smart-open in /usr/local/lib/python3.7/dist-packages (from ray[air]) (5.2.1)\n",
      "Collecting fastapi\n",
      "  Downloading fastapi-0.79.0-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.2 MB/s \n",
      "\u001b[?25hCollecting tensorboardX>=1.9\n",
      "  Downloading tensorboardX-2.5.1-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[K     |████████████████████████████████| 125 kB 61.2 MB/s \n",
      "\u001b[?25hCollecting aiorwlock\n",
      "  Downloading aiorwlock-1.3.0-py3-none-any.whl (10.0 kB)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2022.7.1-py3-none-any.whl (141 kB)\n",
      "\u001b[K     |████████████████████████████████| 141 kB 70.9 MB/s \n",
      "\u001b[?25hCollecting opencensus\n",
      "  Downloading opencensus-0.11.0-py2.py3-none-any.whl (128 kB)\n",
      "\u001b[K     |████████████████████████████████| 128 kB 73.9 MB/s \n",
      "\u001b[?25hCollecting aiohttp-cors\n",
      "  Downloading aiohttp_cors-0.7.0-py3-none-any.whl (27 kB)\n",
      "Collecting uvicorn==0.16.0\n",
      "  Downloading uvicorn-0.16.0-py3-none-any.whl (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.6 MB/s \n",
      "\u001b[?25hCollecting numpy>=1.20\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 44.3 MB/s \n",
      "\u001b[?25hCollecting h11>=0.8\n",
      "  Downloading h11-0.13.0-py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 6.3 MB/s \n",
      "\u001b[?25hCollecting asgiref>=3.4.0\n",
      "  Downloading asgiref-3.5.2-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[air]) (6.0.2)\n",
      "Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[air]) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[air]) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[air]) (1.8.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp>=3.7->ray[air]) (2.1.0)\n",
      "Requirement already satisfied: six>=1.7 in /usr/local/lib/python3.7/dist-packages (from gpustat>=1.0.0b1->ray[air]) (1.15.0)\n",
      "Collecting nvidia-ml-py<=11.495.46,>=11.450.129\n",
      "  Downloading nvidia_ml_py-11.495.46-py3-none-any.whl (25 kB)\n",
      "Collecting psutil>=5.6.0\n",
      "  Downloading psutil-5.9.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (281 kB)\n",
      "\u001b[K     |████████████████████████████████| 281 kB 63.9 MB/s \n",
      "\u001b[?25hCollecting blessed>=1.17.1\n",
      "  Downloading blessed-1.19.1-py2.py3-none-any.whl (58 kB)\n",
      "\u001b[K     |████████████████████████████████| 58 kB 6.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: wcwidth>=0.1.4 in /usr/local/lib/python3.7/dist-packages (from blessed>=1.17.1->gpustat>=1.0.0b1->ray[air]) (0.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[air]) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->ray[air]) (2022.1)\n",
      "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.7/dist-packages (from yarl<2.0,>=1.0->aiohttp>=3.7->ray[air]) (2.10)\n",
      "Collecting starlette\n",
      "  Downloading starlette-0.19.1-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n",
      "\u001b[?25hCollecting anyio<5,>=3.4.0\n",
      "  Downloading anyio-3.6.1-py3-none-any.whl (80 kB)\n",
      "\u001b[K     |████████████████████████████████| 80 kB 10.1 MB/s \n",
      "\u001b[?25hCollecting sniffio>=1.1\n",
      "  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[air]) (0.18.1)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[air]) (5.9.0)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from jsonschema->ray[air]) (4.12.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources>=1.4.0->jsonschema->ray[air]) (3.8.1)\n",
      "Collecting opencensus-context>=0.1.3\n",
      "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from opencensus->ray[air]) (1.31.6)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (1.56.4)\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (57.4.0)\n",
      "Requirement already satisfied: google-auth<2.0dev,>=1.25.0 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (1.35.0)\n",
      "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.7/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (21.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (4.9)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (4.2.4)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=14.3->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (3.0.9)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2.0dev,>=1.25.0->google-api-core<3.0.0,>=1.0.0->opencensus->ray[air]) (0.4.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->ray[air]) (2022.6.15)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->ray[air]) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->ray[air]) (3.0.4)\n",
      "Requirement already satisfied: distlib<1,>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from virtualenv->ray[air]) (0.3.5)\n",
      "Requirement already satisfied: platformdirs<3,>=2.4 in /usr/local/lib/python3.7/dist-packages (from virtualenv->ray[air]) (2.5.2)\n",
      "Building wheels for collected packages: gpustat\n",
      "  Building wheel for gpustat (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gpustat: filename=gpustat-1.0.0rc1-py3-none-any.whl size=18872 sha256=4cdab5a1c4884860e8b314c5b5d5e59ff6cf722a504e4855c43334e55ea2c4c5\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/85/03/7f87ed3a11307c5ad083829b59731788971a8411c265984409\n",
      "Successfully built gpustat\n",
      "Installing collected packages: sniffio, anyio, starlette, psutil, opencensus-context, nvidia-ml-py, numpy, h11, blessed, asgiref, uvicorn, tensorboardX, py-spy, prometheus-client, opencensus, gpustat, fsspec, fastapi, colorful, aiorwlock, aiohttp-cors\n",
      "  Attempting uninstall: psutil\n",
      "    Found existing installation: psutil 5.4.8\n",
      "    Uninstalling psutil-5.4.8:\n",
      "      Successfully uninstalled psutil-5.4.8\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "l5kit 1.5.0 requires numpy~=1.19.0, but you have numpy 1.21.6 which is incompatible.\u001b[0m\n",
      "Successfully installed aiohttp-cors-0.7.0 aiorwlock-1.3.0 anyio-3.6.1 asgiref-3.5.2 blessed-1.19.1 colorful-0.5.4 fastapi-0.79.0 fsspec-2022.7.1 gpustat-1.0.0rc1 h11-0.13.0 numpy-1.21.6 nvidia-ml-py-11.495.46 opencensus-0.11.0 opencensus-context-0.1.3 prometheus-client-0.13.1 psutil-5.9.1 py-spy-0.3.12 sniffio-1.2.0 starlette-0.19.1 tensorboardX-2.5.1 uvicorn-0.16.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "numpy",
         "psutil"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting wandb\n",
      "  Downloading wandb-0.13.1-py2.py3-none-any.whl (1.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.8 MB 5.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
      "Collecting docker-pycreds>=0.4.0\n",
      "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
      "Collecting GitPython>=1.0.0\n",
      "  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n",
      "\u001b[K     |████████████████████████████████| 181 kB 63.1 MB/s \n",
      "\u001b[?25hCollecting setproctitle\n",
      "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
      "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.9.1)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 73.1 MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
      "Collecting shortuuid>=0.5.0\n",
      "  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n",
      "Collecting pathtools\n",
      "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
      "Collecting gitdb<5,>=4.0.1\n",
      "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
      "\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n",
      "Collecting smmap<6,>=3.0.1\n",
      "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n",
      "Collecting sentry-sdk>=1.0.0\n",
      "  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 58.5 MB/s \n",
      "\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 58.8 MB/s \n",
      "\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 55.9 MB/s \n",
      "\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n",
      "\u001b[K     |████████████████████████████████| 157 kB 57.5 MB/s \n",
      "\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n",
      "\u001b[K     |████████████████████████████████| 156 kB 11.5 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pathtools\n",
      "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=b7b090365b6e212cdba0005f54ae368d3a97de5767b4b77096f780757ef437b6\n",
      "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
      "Successfully built pathtools\n",
      "Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n",
      "Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.1\n"
     ]
    }
   ],
   "source": [
    "#@title Download L5 Sample Dataset and install L5Kit\n",
    "#TODO: Place in own step and log to wandb as data logging step\n",
    "import os\n",
    "RunningInCOLAB = 'google.colab' in str(get_ipython())\n",
    "if RunningInCOLAB:\n",
    "    !sh ./setup_notebook_colab.sh\n",
    "    os.environ[\"L5KIT_DATA_FOLDER\"] = open(\"./dataset_dir.txt\", \"r\").read().strip()\n",
    "else:\n",
    "    print(\"Not running in Google Colab.\")\n",
    "    os.environ[\"L5KIT_DATA_FOLDER\"] = \"/tmp/l5kit_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 115
    },
    "executionInfo": {
     "elapsed": 19577,
     "status": "ok",
     "timestamp": 1660683925412,
     "user": {
      "displayName": "Anish Shah",
      "userId": "05913492621931233323"
     },
     "user_tz": 240
    },
    "id": "wk-K5V8tdoWv",
    "outputId": "8a27b81e-ce21-41e2-fba0-f6a62d2ce875"
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
       "            function loadScript(url) {\n",
       "            return new Promise(function(resolve, reject) {\n",
       "                let newScript = document.createElement(\"script\");\n",
       "                newScript.onerror = reject;\n",
       "                newScript.onload = resolve;\n",
       "                document.body.appendChild(newScript);\n",
       "                newScript.src = url;\n",
       "            });\n",
       "            }\n",
       "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
       "            const iframe = document.createElement('iframe')\n",
       "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
       "            document.body.appendChild(iframe)\n",
       "            const handshake = new Postmate({\n",
       "                container: iframe,\n",
       "                url: 'https://wandb.ai/authorize'\n",
       "            });\n",
       "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
       "            handshake.then(function(child) {\n",
       "                child.on('authorize', data => {\n",
       "                    clearTimeout(timeout)\n",
       "                    resolve(data)\n",
       "                });\n",
       "            });\n",
       "            })\n",
       "        });\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "··········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CHPqOPJ9DmR1"
   },
   "outputs": [],
   "source": [
    "from tempfile import gettempdir\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "\n",
    "from l5kit.configs import load_config_data\n",
    "from l5kit.data import LocalDataManager, ChunkedDataset\n",
    "from l5kit.dataset import AgentDataset\n",
    "from l5kit.rasterization import build_rasterizer\n",
    "from l5kit.geometry import transform_points\n",
    "from l5kit.visualization import TARGET_POINTS_COLOR, draw_trajectory\n",
    "from l5kit.planning.rasterized.model import RasterizedPlanningModel\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WT_G_ukODmR2"
   },
   "source": [
    "## Prepare data path and load cfg\n",
    "\n",
    "By setting the `L5KIT_DATA_FOLDER` variable, we can point the script to the folder where the data lies.\n",
    "\n",
    "Then, we load our config file with relative paths and other configurations (rasteriser, training params...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Evg6xoIDmR2"
   },
   "outputs": [],
   "source": [
    "# get config\n",
    "cfg = load_config_data(\"./config.yaml\")\n",
    "sample = True\n",
    "if sample:\n",
    "    cfg[\"train_data_loader\"][\"key\"] = \"scenes/sample.zarr\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gC6qm8K5DmR2"
   },
   "outputs": [],
   "source": [
    "# # plot some examples\n",
    "# for idx in range(0, len(train_dataset), len(train_dataset) // 10):\n",
    "#     data = train_dataset[idx]\n",
    "#     im = rasterizer.to_rgb(data[\"image\"].transpose(1, 2, 0))\n",
    "#     target_positions = transform_points(data[\"target_positions\"], data[\"raster_from_agent\"])\n",
    "#     draw_trajectory(im, target_positions, TARGET_POINTS_COLOR)\n",
    "#     plt.imshow(im)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byNycn-PDmR3"
   },
   "source": [
    "# Prepare for training\n",
    "Our `AgentDataset` inherits from PyTorch `Dataset`; so we can use it inside a `Dataloader` to enable multi-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ua0D8twDmR3"
   },
   "outputs": [],
   "source": [
    "import ray.train as train\n",
    "from ray.air import session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8dGyI8IL3xz"
   },
   "outputs": [],
   "source": [
    "def train_simulation_model(cfg: dict):\n",
    "\n",
    "    dm = LocalDataManager(None)\n",
    "\n",
    "    # rasterisation\n",
    "    rasterizer = build_rasterizer(cfg, dm)\n",
    "\n",
    "    # ===== INIT DATASET\n",
    "    train_zarr = ChunkedDataset(dm.require(cfg[\"train_data_loader\"][\"key\"])).open()\n",
    "    train_dataset = AgentDataset(cfg, train_zarr, rasterizer)\n",
    "\n",
    "\n",
    "    model_arch = cfg[\"model_params\"][\"model_architecture\"]\n",
    "    future_num_frames = cfg[\"model_params\"][\"future_num_frames\"]\n",
    "\n",
    "    train_cfg = cfg[\"train_data_loader\"]\n",
    "    shuffle = train_cfg[\"shuffle\"]\n",
    "    batch_size = train_cfg[\"batch_size\"]\n",
    "    num_workers = train_cfg[\"num_workers\"]\n",
    "\n",
    "    max_num_steps = cfg[\"train_params\"][\"max_num_steps\"]\n",
    "\n",
    "    batch_size_per_worker = batch_size // session.get_world_size()\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset, shuffle=shuffle, batch_size=batch_size_per_worker, num_workers=num_workers)\n",
    "    train_dataloader = train.torch.prepare_data_loader(train_dataloader)\n",
    "\n",
    "    model = RasterizedPlanningModel(\n",
    "        model_arch=model_arch,\n",
    "        num_input_channels=rasterizer.num_channels(),\n",
    "        num_targets=3 * future_num_frames,  # X, Y, Yaw * number of future states,\n",
    "        weights_scaling= [1., 1., 1.],\n",
    "        criterion=nn.MSELoss(reduction=\"none\")\n",
    "    )\n",
    "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # model = model.to(device)\n",
    "\n",
    "    model = train.torch.prepare_model(model)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    tr_it = iter(train_dataloader)\n",
    "    progress_bar = tqdm(range(max_num_steps))\n",
    "    losses_train = []\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "\n",
    "    for _ in progress_bar:\n",
    "        try:\n",
    "            data = next(tr_it)\n",
    "        except StopIteration:\n",
    "            tr_it = iter(train_dataloader)\n",
    "            data = next(tr_it)\n",
    "        data = {k: v for k, v in data.items()} #Quick hack to access data TODO: Replace with proper data loading with DDP\n",
    "        loss = train_simulation_model_epoch(data, model, optimizer)\n",
    "        losses_train.append(loss.item())\n",
    "        progress_bar.set_description(f\"loss: {loss.item()} loss(avg): {np.mean(losses_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w1JPA9D9NeZO"
   },
   "outputs": [],
   "source": [
    "def train_simulation_model_epoch(data, model, optimizer):\n",
    "     # Forward pass        \n",
    "    result = model(data)\n",
    "    loss = result[\"loss\"]\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SmAarVEdQK5q"
   },
   "outputs": [],
   "source": [
    "from ray.train.torch import TorchTrainer\n",
    "from ray.air.config import RunConfig, ScalingConfig\n",
    "from ray.air.callbacks.wandb import WandbLoggerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9MLY1WNd40u"
   },
   "outputs": [],
   "source": [
    "wandb_project = \"level-5-sim\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWW_BdFXPm4K"
   },
   "outputs": [],
   "source": [
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_simulation_model,\n",
    "    train_loop_config=cfg,\n",
    "    scaling_config=ScalingConfig(num_workers=1, use_gpu=True), #TODO: Add logic to check if GPU is available here\n",
    "    run_config=RunConfig(\n",
    "            callbacks=[\n",
    "                # This is the part needed to enable logging to Weights & Biases.\n",
    "                # It assumes you've logged in before, e.g. with `wandb login`.\n",
    "                WandbLoggerCallback(\n",
    "                    project=wandb_project,\n",
    "                    save_checkpoints=False,\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "zS1Jj185c2GX",
    "outputId": "4965fe70-3422-4f25-f44c-d3222a7cddf4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-16 21:08:56,903\tINFO wandb.py:119 -- Already logged into W&B.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "== Status ==<br>Current time: 2022-08-16 21:14:16 (running for 00:05:19.86)<br>Memory usage on this node: 2.2/12.7 GiB<br>Using FIFO scheduling algorithm.<br>Resources requested: 1.0/2 CPUs, 1.0/1 GPUs, 0.0/7.32 GiB heap, 0.0/3.66 GiB objects (0.0/1.0 accelerator_type:P100)<br>Result logdir: /root/ray_results/TorchTrainer_2022-08-16_21-08-56<br>Number of trials: 1/1 (1 RUNNING)<br><table>\n",
       "<thead>\n",
       "<tr><th>Trial name              </th><th>status  </th><th>loc           </th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>TorchTrainer_a424f_00000</td><td>RUNNING </td><td>172.28.0.2:931</td></tr>\n",
       "</tbody>\n",
       "</table><br><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m\u001b[36m(RayTrainWorker pid=1019)\u001b[0m 2022-08-16 21:12:02,949\tINFO config.py:72 -- Setting up process group for: env:// [rank=0, world_size=1]\n"
     ]
    }
   ],
   "source": [
    "result = trainer.fit()\n",
    "print(f\"Last result: {result.metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZYx6BpaDmR3"
   },
   "source": [
    "# Training loop\n",
    "Here, we purposely include a barebone training loop. Clearly, many more components can be added to enrich logging and improve performance, such as:\n",
    "- learning rate drop;\n",
    "- loss weights tuning;\n",
    "- importance sampling\n",
    "\n",
    "To name a few.\n",
    "\n",
    "\n",
    "Still, the sheer size of our dataset ensures that a reasonable performance can be obtained even with this simple loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 200,
     "status": "ok",
     "timestamp": 1660684978450,
     "user": {
      "displayName": "Anish Shah",
      "userId": "05913492621931233323"
     },
     "user_tz": 240
    },
    "id": "3bSUQ0bVciE4",
    "outputId": "439a8a2e-2b75-4c86-c1bf-dc1c24c5f7e6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Result(metrics={'trial_id': 'a424f_00000', 'done': True}, error=None, log_dir=PosixPath('/root/ray_results/TorchTrainer_2022-08-16_21-08-56/TorchTrainer_a424f_00000_0_2022-08-16_21-08-59'))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5-QIdWC-DmR4"
   },
   "source": [
    "### Plot the train loss curve\n",
    "We can plot the train loss against the iterations (batch-wise) to check if our model has converged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeClsoIKDmR4"
   },
   "outputs": [],
   "source": [
    "# plt.plot(np.arange(len(losses_train)), losses_train, label=\"train loss\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v59DvHGPDmR4"
   },
   "source": [
    "# Store the model\n",
    "\n",
    "Let's store the model as a torchscript. This format allows us to re-load the model and weights without requiring the class definition later.\n",
    "\n",
    "**Take note of the path, you will use it later to evaluate your planning model!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLPr6bYZDmR4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# to_save = torch.jit.script(model.cpu())\n",
    "# path_to_save = f\"{gettempdir()}/simulation_model.pt\"\n",
    "# to_save.save(path_to_save)\n",
    "# print(f\"MODEL STORED at {path_to_save}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L590Lk2GDmR4"
   },
   "source": [
    "# Congratulations in training your first ML policy for simulation!\n",
    "### What's Next\n",
    "\n",
    "Now that your model is trained and safely stored, you can use it to control the agents around ego. We have a notebook just for that.\n",
    "\n",
    "### [Simulation evaluation](./simulation_test.ipynb)\n",
    "In this notebook a `planning_model` will control the SDV, while the `simulation_model` you just trained will be used for all other agents.\n",
    "\n",
    "Don't worry if you don't have the resources required to train a model, we provide pre-trained models just below.\n",
    "\n",
    "## Pre-trained models\n",
    "we provide a collection of pre-trained models for the simulation task:\n",
    "- [simulation model](https://lyft-l5-datasets-public.s3-us-west-2.amazonaws.com/models/simulation_models/simulation_model_20210416_5steps.pt) trained on agents over the semantic rasteriser with history of 0.5s;\n",
    "- [planning model](https://lyft-l5-datasets-public.s3-us-west-2.amazonaws.com/models/simulation_models/planning_model_20210421_5steps.pt) trained on the AV over the semantic rasteriser with history of 0.5s;\n",
    "\n",
    "To use one of the models simply download the corresponding `.pt` file and load it in the evaluation notebooks."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Level-5-ML-Sim-Torch.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/lyft/l5kit/blob/master/examples/simulation/train.ipynb",
     "timestamp": 1660677417264
    }
   ]
  },
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m95"
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
